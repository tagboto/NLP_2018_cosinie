{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Group Project\n",
    "## Topic Modelling - Latent Dirchelet Allocation Implementation\n",
    "\n",
    "Name: Zoe Tagboto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of the necessary libraries necessary to run this implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re \n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import codecs\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using panda dataframes to read in the Data. This will make our lives way easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    question_data = pd.read_csv('Questions.txt', sep='\\t', names=['questions'], index_col= None, header = 0 )\n",
    "    topics_data = pd.read_csv('Topics.txt', sep='\\t', names=['topics'], index_col= None, header = 0 )\n",
    "        \n",
    "    #After getting the data I combine them into 1 data frame to get questions and topics\n",
    "    q_t_data = pd.concat([question_data,topics_data],axis=1)#.iloc[:].values\n",
    "    \n",
    "    return q_t_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(q_t_data):\n",
    "    question_data = q_t_data.questions.values.tolist()\n",
    "    question_data = [re.sub('\\s+', ' ', sent) for sent in question_data]\n",
    "    question_data = [re.sub(\"\\'\", \"\", sent) for sent in question_data]\n",
    "    \n",
    "    for sentence in question_data:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalize(question):\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100) \n",
    "    stop_words = stopwords.words('english')\n",
    "    remove_stop =[[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in question]\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    #trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    bigrams = [bigram_mod[doc] for doc in remove_stop]\n",
    "    #trigrams = [trigram_mod[bigram_mod[doc]] for doc in remove_stop]\n",
    "    \n",
    "    \n",
    "    data_lemmatized = lemmatization(bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "\n",
    "    return data_lemmatized\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_and_corpus(normalized_text):\n",
    "    id2word = corpora.Dictionary(normalized_text)\n",
    "    texts = normalized_text\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "   \n",
    "    return id2word, corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(id2word, corpus):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    #pprint(lda_model.print_topics())\n",
    "    doc_lda = lda_model[corpus]\n",
    "    \n",
    "    return(doc_lda)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_coherence(corpus, doc_lda, normalized_data, id2word):\n",
    "    print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(lda_model, corpus, id2word):\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zoe Tagboto\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'visualization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-7ca7c951ace3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdoc_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mvisualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mmodel_coherence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_lda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'visualization' is not defined"
     ]
    }
   ],
   "source": [
    "#q_t = read_data()\n",
    "q_t_data = read_data()\n",
    "data_words = list(clean_data2(q_t_data))\n",
    "normalized_data = data_normalize(data_words)\n",
    "\n",
    "id2word, corpus = dict_and_corpus(normalized_data)\n",
    "doc_lda = lda_model(id2word,corpus)\n",
    "\n",
    "visualization(lda_model, corpus, id2word)\n",
    "model_coherence(corpus, doc_lda, normalized_data, id2word)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
